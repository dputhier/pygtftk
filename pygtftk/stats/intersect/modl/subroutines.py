"""
Pure subroutines of the MODL algorithm.

Author : Quentin Ferr√© <quentin.q.ferre@gmail.com>
"""

from collections import defaultdict

import numpy as np 
import pandas as pd
import sklearn
import pybedtools

from sklearn.decomposition import SparseCoder
from sklearn.decomposition import MiniBatchDictionaryLearning
from sklearn.naive_bayes import GaussianNB

from pygtftk.stats.intersect.modl import tree
import pygtftk.stats.intersect.overlap_stats_compute as osc     # Import the module directly to avoid circular imports
from pygtftk.stats.intersect.read_bed import read_bed_as_list as read_bed

from pygtftk.utils import message



def learn_dictionary_and_encode(data, n_atoms = 20, alpha = 0.5,
        n_iter = 200, random_seed = 42, n_jobs = 1,
        fit_algorithm = 'cd',
        transform_algorithm = 'lasso_cd'):
    r"""
    Will learn a dictionary for the data (row-wise) and encode it, with the specified parameters.
    Returns the dictionary, components as a dataframe, and encoded data.

    By default, allows 20 words with alpha of 0.5.

    More info at : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html

    >>> from pygtftk.stats.intersect.modl.dict_learning import test_data_for_modl
    >>> from pygtftk.stats.intersect.modl.subroutines import learn_dictionary_and_encode
    >>> import numpy as np
    >>> np.random.seed(42)
    >>> flags_matrix = test_data_for_modl(nflags = 1000, number_of_sets = 6)
    >>> import time
    >>> start_time = time.time()
    >>> U_df, V_df, error = learn_dictionary_and_encode(flags_matrix, n_atoms = 20, alpha = 0.5)
    >>> stop_time = time.time()

    """


    # TODO: Many operations here, such as recreating an object of a pandas 
    # dataframe, might not be necessary ?
    
    data = np.array(data) # Force cast as array

    # Cannot ask for more rows than there are unique lines
    n_atoms = min(n_atoms, len(np.unique(data, axis = 0)))


    dico = MiniBatchDictionaryLearning(n_components=n_atoms, n_iter=n_iter,
        alpha = alpha,
        fit_algorithm = fit_algorithm, 
        transform_algorithm = transform_algorithm, transform_alpha = alpha, 
        positive_dict = True, positive_code = True,
        random_state = random_seed,
        n_jobs = n_jobs 
        ) 

    # NOTE fit_algorithm is used during the learning and transform_algorithm 
    # transforms the data once the estimator has been fitted.
    # We are using coordinate descent (CD) as LARS has troubles with correlated
    # features. Also because we want to be able to enforce positivity in the 
    # dictionary and the code for interpretability.

    dico.fit(data)  # Fit the data

    # Get components (the dictionary).
    # NOTE: Use a try-except for future proofing, as sklearn (v 0.24) seems to 
    # deprecate 'components_' across the board and replaced it with 'dictionary'
    try: V = dico.components_
    except: V = dico.dictionary  
    V_df = pd.DataFrame(V)


    # If the alpha is inadapted, the learned dictionary may have failed to converge
    # and contain NaNs. If that happens, return it now and bypass the next steps
    # NOTE LARS is less vulnerable to it, use it as a fallback before calling it quits.
    if np.isnan(V).any():
        message("Learned dictionary by Coordinate descent contained NaNs. Alpha may have been indadapted. Defaulting to LARS.", type = 'DEBUG')
        
        dico = MiniBatchDictionaryLearning(n_components=n_atoms, n_iter=n_iter,
            alpha = alpha, transform_alpha = alpha, random_state = random_seed, n_jobs = n_jobs,
            fit_algorithm = 'lars',  transform_algorithm = 'lasso_lars') 

        dico.fit(data)
        try: V = dico.components_
        except: V = dico.dictionary  
        V_df = pd.DataFrame(V)
                
        # Only abort if it still does not work
        if np.isnan(V).any():
            message("Fallback still contains NaNs. Aborting.", type = 'DEBUG')
            return None, V_df, None # Return "U", V, "error"


    # Re-encode the data with this dictionary
    encoded_data = dico.transform(data)
    ed_df = pd.DataFrame(encoded_data)

    # Compute associated normalized L2 loss for reference
    reconstructed_features = np.matmul(encoded_data,V)
    error = np.sum((reconstructed_features-data)**2) / np.sum(data**2)

    return ed_df, V_df, error # Return U, V, error






# ---------------------------------------------------------------------------- #
#                        Step 1 of the MODL algorithm                          #
# ---------------------------------------------------------------------------- #

def generate_candidate_words(X, n_words, nb_threads = 1,
    discretization_threshold=0, alphas=None):
    """
    Given a data matrix, will generate a library of candidate words by trying many
    different Dictionary Learnings (different alphas only for now)
    """

    EPSILON = 1E-10

    ## Preliminary steps
    X = np.array(X) # Force cast to array for safety
    nb_features = X.shape[1] # word size

    # NOTE : n_atoms cannot be higher than the number of unique combis or there might be an Exception thrown.
    # It seems to vary and be due to alpha. To be safe, prevent it.
    nb_unique_combis = len(np.unique(X, axis = 0))
    if n_words > nb_unique_combis : n_words = nb_unique_combis

   
    # A too low alpha can result in learning giberrish words, thay may still be used as there is no penalty for it.
    # To counter this, we begin with an alpha of 1/nb_features
    # Rk: if alphas have been manually specified, use them instead
    if alphas is None: alpha = 1/nb_features
    else: alpha = alphas[0]

    # Remember all candidate words found at each step, and use a dictionary so we
    # can remember their total usage for later filtering
    all_candidate_words = defaultdict(lambda: 0)

    stop = False

    misbehaving = 0

    iternb = 0
    while not stop:

        # Encode using DL     
        #start_time = time.time()
        message("> Alpha is "+str(alpha), type = 'DEBUG')

        # NOTE For some reason, the algorithm will never learn the word (1,1,1,...,1,1) even if it the only row in the data.
        # To prevent that, add a useless padding for the last column.
        pad = np.zeros((X.shape[0],1))
        Xpadded = np.append(X, pad, axis=1)

        # Perform the learning
        U, V, error = learn_dictionary_and_encode(Xpadded, n_atoms = n_words, alpha = alpha,
                                               n_jobs = nb_threads)

        #stop_time = time.time() # Careful not to name it 'stop' and overwrite the stop flag :)
        #print('One DL step took ', str(stop_time-start_time), 'seconds.')

        iternb += 1
        # Update alpha so DL will be more sparse in the next step
        # and look for higher-order combinations
        # We add iternb/nb_features to not take too long and not unduly favor high alphas (and longer words)
        
        # Rk: if alphas have been manually specified, use them instead
        if alphas is None: alpha += iternb/nb_features
        else:
            try: alpha = alphas[iternb]
            except: stop = True # Stop if we reached the end of the list
        
        
        # TODO: Also stop regardless after 2*k iterations ?

        # We stop once alpha is too high and preventing any word from being used
        # in the encoding U (which contained NaNs -- handled before -- or has a sum of 0)
        # Rq : can also happen with too low alpha or convergence errors, so stop only if it happens more than 2 times
        if (U is None): misbehaving += 1
        if (misbehaving >=2): stop = True
            
        if U is not None:

            if np.sum(U.values) == 0:
                stop = True
                continue # Do not do the following steps of word extraction !! The stop signal has been sent, and if U is None there is nothing to extract



            # -- Extract interesting words -- #
            # Here extract interesting words from dico and add to all_found_words

            # Binarize them (ie transform 0.9 - 0.9 - 0.001 into 1-1-0)
            # The sum of V_df**2 for each word is always 1.0 so our cutoff to say if a feature is used
            # is V**2 > 1/n_features**2 - epsilon for rounding errors. 
            # We cannot simply binarize, since 0.001 would be binarized to 1 yet it is likely not interesting.
            # WARNING : no longer true if I have intra set overlap (ie 1-1-2 for example) so when looking for the combis I might need to ignore intra set overlap

            # Each learned word is remembered along with its usage in the reconstruction
            # This way when proceeding to step 2 we keep only the most relevant words from step 1 to save time.
            # Gibberish overcomplete words that are found when n_atoms is too high and were not used will have usage of 0
            for i in U.columns.tolist():
                this_word = (V**2).iloc[i,:].values

                # Remove padding
                this_word = this_word[:-1]

                # Above 1/k**2 ?
                first_thres = this_word - (1/nb_features**2) > EPSILON

                # Above discretization_threshold * max ?
                second_thres = this_word - (discretization_threshold * max(this_word)) > EPSILON

                # Above both ?
                this_word_binarized = np.logical_and(first_thres,second_thres).astype(int)


                this_word_binarized = tuple(this_word_binarized.tolist())

                this_word_usage = np.sum(U.values[:,i])

                # Now remember it
                all_candidate_words[this_word_binarized] += this_word_usage

                # Debug print
                message("Word = "+str(this_word_binarized)+" ; Usage = "+str(this_word_usage),
                    type = 'DEBUG')



    # Return the dictionary giving all candidate words and their total usage
    return all_candidate_words




# ---------------------------------------------------------------------------- #
#                        Step 2 of the MODL algorithm                          #
# ---------------------------------------------------------------------------- #

def normalize_and_jitter_matrix_rows(X, normalize = True, jitter = True):
    r"""
    Apply normalization so that sum of suquare of each row is 1,
    Add a slight epsilon so two rows may not have exact same dot product with a given vector
    Sort in lexicographic order

    >>> import numpy as np
    >>> import numpy.testing as npt
    >>> from pygtftk.stats.intersect.modl.subroutines import normalize_and_jitter_matrix_rows
    >>> D = np.array([[1,1,0],[0,1,0],[0,1,1]])
    >>> Dc = normalize_and_jitter_matrix_rows(D)
    >>> Dc_theory = [[0, 1, 0], [7.0736E-5, 7.07107E-1, 7.07107E-1], [7.07107E-1, 7.07107E-1, 9.99859E-05]]
    >>> npt.assert_almost_equal(Dc, Dc_theory, decimal = 5)
    """

    X = np.array(X) # Enforce NumPy array
    X = X[np.lexsort(np.rot90(X))] # Sort the dictionary in lexicographic order BEFORE applying all of this

    ## Applying corrections to the words : normalize and jitter
    new_X = []
    i = 0
    for row in X: 
        
        # To prevent atoms in D from having the exact same dot product with any word
        # that LARS will try to rebuild, add a very small jitter to the words 
        # -> sqrt(i) times 1E-4 to each element of the i-th row of D
        neo = np.array([x + np.sqrt(i)*1E-4 for x in row])
        i += 1        

        # Normalize the words by the sum of their square. Helps counter a tendency to select longer words, which can increase sensitivity to noise.
        if normalize:
            squared_sum = np.sum(neo**2)
            if squared_sum > 0: # Avoid division by zero
                neo = np.sqrt(neo**2/squared_sum)

        new_X += [neo]

    # Now convert to array
    X = np.array(new_X)
    
    return X



def build_best_dict_from_library(data, library, queried_words_nb,
                error_function = None,
                nb_threads = 1, normalize_words = False,
                transform_alpha = None):
    r"""
    Given a data matrix and a library, will select the best n = queried_words_nb words
    with a greedy algorithm from the library to rebuild the data.

    `data` is a matrix with one transaction per row and one element per column, as usual

    The greeedy algorithm works because the problem in this particular instance
    is submodular. In broad strokes, it is assumed that the gain in term of reconstruction
    with the Sparse Coder of adding the word X to a set S is no higher than adding X to a
    set T that contains S. Also, more words cannot make the sparse coder do a worse job so
    the function is monotonous.

    Instead of the reconstruction error, you may pass a different callable of the form
    error_function(data, rebuilt_data, encoded, dictionary) that returns an error value so there
    can be a supervision, but the submodularity might no longer hold.

    >>> import numpy as np
    >>> from pygtftk.stats.intersect.modl import tree
    >>> from pygtftk.stats.intersect.modl import subroutines as modl_subroutines
    >>> X = np.array([[1,1,0,0,0,0],[1,1,1,1,0,0],[0,0,0,0,1,1]]*10)
    >>> candidates = [(1,1,0,0,0,0),(1,1,1,1,0,0),(0,0,0,0,1,1), (1,1,0,0,1,1), (1,0,1,0,1,0),(1,1,0,1,1,0),(1,0,1,1,0,0),(0,1,0,0,1,1)]
    >>> L = tree.Library()
    >>> L.build_nodes_for_words(candidates)
    >>> L.assign_nodes()
    >>> selection = modl_subroutines.build_best_dict_from_library(X, L, queried_words_nb = 3)
    >>> assert set(selection) == set([(1,1,0,0,0,0),(1,1,1,1,0,0),(0,0,0,0,1,1)])

    """


    ## Create objects

    # Initial dictionary
    nb_features = data.shape[1]

    dictionary = [tuple([0] * nb_features)] # Must initialize with a word, might as well be full zeros.

    # Use nonzero alpha for the coder as well, only if not manually specified.
    if transform_alpha is None:
               
        # To encourage use of fewer words, we by default use a rather high alpha (but capped at 0.5)
        transform_alpha = min(1/np.sqrt(nb_features), 0.5)




    ## Coder object, to be updated iteratively
    coder = SparseCoder(dictionary=np.array(dictionary).astype('float64'), # Rq : must make it float64 for some reason
        transform_algorithm='lasso_lars', #transform_algorithm='lasso_cd',
        transform_alpha = transform_alpha,
        positive_code = True, # Very important to ensure we also force positivity here !
        n_jobs = nb_threads)

    # We use lasso_lars, as lasso_cd often had mysterious convergence problems.
    # Lasso_lars can stil enforce positivity, and with precomputed words now the poor handling of degenerate
    # vectors (lars dropping them) is a strength


    # --- Filtering the candidates
    # Iteratively add candidates that improve the reconstruction
    stop = False
    while stop is False:

        ## Generate all_candidate_words for this iteration
        # For now, simply all words of the library that are not already used
        all_candidate_words = tree.get_all_candidates_except(library, dictionary)

        # Test all words for this iteration
        candidates_with_errors = dict()

        message("----------------------", type = 'DEBUG')
        message("Current dictionary : "+str(dictionary), type = 'DEBUG')
        message("> Candidate\tError", type = 'DEBUG')

        for candidate in all_candidate_words:

            # Create a new dictionary for this test
            dict_being_tested = dictionary + [candidate]

            Dt = np.array(dict_being_tested)
            
            # Normalization helps counter a tendency to selcet longer words, and jitter fixes a LARS bug with atoms that have the same dot product
            # Also lexicographic sort.
            Dt_corrected = normalize_and_jitter_matrix_rows(Dt, normalize_words)
            Dt = Dt_corrected
            
            # Update dictionary to the one being currently tested
            coder.dictionary = Dt.astype('float64')

            # Just to be safe
            assert np.allclose(coder.dictionary, Dt.astype('float64'))


            try:

                encoded = coder.transform(data)
            
                rebuilt_data = np.matmul(encoded, Dt)

                # We use Manhattan error so compromises are discouraged : with L2,
                # when rebuilding [(1,0,0)*many,(1,0,1),(0,0,1)*many], (1,0,1) would be picked first
                # Add an alpha that is nonzero so that using longer words is still
                # an improvement even when other words cover it (and as a tiebreaker)
                # but keep it low (1/k) so you don't re-encourage compromise
                def manhattan_dist_with_sparsity(X_true, X_rebuilt, encoded, dictionary):

                    dictionary = None # This particular function ignores the dictionary

                    error_mat = np.abs(X_rebuilt-X_true)
                    error = np.sum(error_mat)

                    alpha = transform_alpha # Fetch the alpha used when calling the main function

                    regul = np.sum(encoded) * alpha

                    final_error = error + regul
                    return final_error


                # Check for error function, default is manhattan dist
                if error_function is None:
                    error_function = manhattan_dist_with_sparsity

                # To compute the error, pass respectively : true data, rebuilt data, encoded data, and current dictionary 
                # TODO: currently, the dictionary passed is BEFORE normalization and all that jazz 
                error = error_function(data, rebuilt_data, encoded, dict_being_tested)

            # On rare occasion, convergence errors can result in a ValueError
            except ValueError:
                error = np.inf

            # Remember error
            candidates_with_errors[tuple(candidate)] = error

            message(str(candidate)+'\t'+str(error), type = 'DEBUG')

      
        # Add best candidate to final dict and remove from candidates.
        try:
            # In case of a tie, the first word is selected.
            best_candidate = min(candidates_with_errors, key=candidates_with_errors.get)
            dictionary += [best_candidate]
        # It should never happen, but if there are no more candidates add an empty word.
        except:
            dictionary += [tuple([0] * nb_features)]


        """ 
        # NOTE for improvement
        # To save time, we may only pass as candidates the "children" (see tree module)
        # of nodes already present under the assumption that if X is better than Y
        # the best parent of X should be better than the best parent of Y ?
        # Then we would need to wait for an optimum, with this draft code

        # If we have more words than the user wants, remove the worst one
        if (len(dictionary) > queried_words_nb):
            # Remove worst word with respect to error (try all, opposite of above)

        # When the word added and the word removed are the same (meaning
        # the word it wants to add does not improve on the dict) stop and
        # return
        if best_candidate == last_word_removed :
            best_words = final_dict
            return best_words
        """

        ## Stop condition : as soon as we have enough words
        # Remark : since the dictionary always contain the (0,0,0,...) word as a supplement, we
        # must stop when it contains the required number of words... plus one !
        if (len(dictionary) >= queried_words_nb + 1):
            stop = True




    message("Final dictionary :"+str(dictionary), type = 'DEBUG')

    # Conclusion of the function : remove the (0,0,0,...) word and make words unique, just in case
    best_words = list(set(dictionary))
    best_words.remove(tuple([0] * nb_features))

    return best_words




# ---------------------------------------------------------------------------- #
#                        Gaussian Naive Bayes                                  #
# ---------------------------------------------------------------------------- #


def presence_vector(row, combis):
    """
    Given a list of combinations, returns a vector saying whether they are
    present or absent in this row. The order in the vector is the same as given
    in the combis list.

    This works exactly like an inexact (transitive) count, which is OLOGRAM-MODL's default operating mode.

    Example:

    >>> row = [1,0,1,1,0,1]
    >>> combis = [1,0,1,0,0,0],[0,1,1,0,0,0]
    >>> res = presence_vector(row, combis)
    >>> assert list(res) == [1,0]

    """

    result = []
    for combi in combis:
        mask = row - combi

        # If any element from the combi is missing, it's absent, so
        # add a 0. Otherwise add a 1.
        if min(mask) < 0 : result +=[0]
        else: result += [1]

    return result


def prepare_matrix_for_gnb(KEEP_N_TIMES_QUERY, bedA, bedsB, chrom_len, bed_excl):
    """
    Prepare intersection matrices between all reference sets, inclusing positions where the query is absent.
    See the comments for details.

    Subsampling is applied and controlled with this parameter:
    - KEEP_N_TIMES_QUERY = For each '1' line with the query, how many '0' lines without it do we keep?
    """
    
    # Generate a fake bed for the entire genome, using the chromsizes  
    full_genome_bed = [str(chrom) + '\t' + '0' + '\t' + str(chrom_len[chrom]) + '\n' for chrom in chrom_len if chrom != 'all_chrom']
    full_genome_bed = pybedtools.BedTool(full_genome_bed)

    # Also do the exclusion manually if needed
    if bed_excl is not None:
        full_genome_bed_after_excl = read_bed.exclude_concatenate(full_genome_bed, bed_excl)
    else:
        full_genome_bed_after_excl = full_genome_bed

    # Note that by definition, in this intersections' matrix only regions where at 
    # least two sets are open are given. For example {4} alone is not found. 
    # To fix it, use a fake full genome bed as query, so there is always one file 
    # open, then truncate it to get the flags_matrix. 
    true_intersection = osc.compute_true_intersection(full_genome_bed_after_excl, [bedA] + bedsB)
    flags_matrix = np.array([i[3] for i in true_intersection])

    # with np.printoptions(threshold=np.inf): print(flags_matrix)
    flags_matrix = flags_matrix[:,1:] # Remove first column that represents the fake, full genome BED.

    """
    # NOTE : The length of each intersection is also accessible, in case you want
    # the final matrix to have one row per X base pairs instead of 1 row per intersection/event.
    # For reference, the true_intersection object looks like this: [('chr1', 150, 180, np.array([1,1,0])), ('chr1', 180, 200, np.array([1,1,1])), ('chr1', 350, 380, np.array([1,1,0]))]
    for i in true_intersection: # For each observed intersection...
        inter_chr, inter_start, inter_stop = i[0], i[1], i[2]   # Get the chromosome, start and end of the observed intersection
        intersection_length = inter_stop - inter_start          # Deduce the length of the intersection
        intersection_flags = i[3]                               # Which combination was observed here?
        # Now you can process it.
    """

    # KEEP_N_TIMES_QUERY = For each '1' line with the query, how many '0' lines without it do we keep?

    # Perform subsampling of the majority class ('0', meaning query is absent)
    flags_matrix_with_query = flags_matrix[flags_matrix[:, 0] != 0]
    draw_this_many = int(np.around(KEEP_N_TIMES_QUERY * len(flags_matrix_with_query)))
    random_rows = flags_matrix[np.random.randint(flags_matrix.shape[0], size=draw_this_many),:]
    flags_matrix = np.concatenate((flags_matrix_with_query,random_rows))

    # Keep only combis WITH query (first element is not 0)
    flags_matrix_with_query = flags_matrix[flags_matrix[:, 0] != 0]

    return flags_matrix, flags_matrix_with_query


# This is the custom error function that we will use
def custom_error_function_bnb(X_true, X_rebuilt, encoded, dictionary,
        flags_matrix, QUERY_WEIGHT):
    """
    This is function that only cares about the dictionary and discards X_true in 
    favor of the previous flags_matrix.
    """

    # Always assume that the first column is the query, so we split it.
    # Truncate the first column, with the query, since that is what we want to predict
    Y = flags_matrix[:,0]
    X = flags_matrix[:,1:]

    new_dictionary = []
    for atom in dictionary:
        new_dictionary += [atom[1:]]
    dictionary = np.array(new_dictionary)


    # Convert to a binary matrix giving, for each atom in the dictionary, 
    # whether it is present at this row
    X_bydict_bin = [presence_vector(row, dictionary) for row in X]
    X_bydict_bin = np.array(X_bydict_bin)

    # Use Gaussian Naive Bayes on the data and retrieve accuracy
    bnb = GaussianNB()

    # Potential sample weights
    sample_weights = []
    for y in Y:
        # In the Naive Bayes classifier, how much more do we weigh the presence of the query?
        if y == 1 : sample_weights += [QUERY_WEIGHT]
        else: sample_weights += [1]
    
    bnb.fit(X_bydict_bin, Y, sample_weight= sample_weights)
    Y_pred = bnb.predict(X_bydict_bin)

    # Compute F1-score
    score = sklearn.metrics.f1_score(Y, Y_pred)
        
    return -score # We want an error, higher means worse
